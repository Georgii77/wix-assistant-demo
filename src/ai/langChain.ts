import "dotenv/config";
import { ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { Runnable, RunnableSequence } from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers";
import type { RedisClientType} from 'redis';
import * as fs from "fs";
import logger from '../utils/logger.ts';



export default async function incomingLeadChain(data: any): Promise<string> {

  const systemPrompt = fs.readFileSync('src/static/systemLeadPrompt.txt', 'utf-8');

  const prompt = ChatPromptTemplate.fromMessages([
  ["system", systemPrompt],
  ["user",   "Here is the lead data:\n{lead}\n\nWrite  reply."]
]);


const llm = new ChatOpenAI({
  apiKey: process.env.OPENAI_API_KEY,
  model:  "gpt-5-mini",
});


const toText = new StringOutputParser();


const chain = RunnableSequence.from([prompt, llm, toText]);



try{
  logger.info("retrieving examples and lead data")
  const examples = fs.readFileSync('src/static/examples.txt', 'utf-8');
  const lead: string = JSON.stringify(data, null, 2)
  logger.info("LangChain generating response")
  const replyText: string = await chain.invoke({examples: examples, lead: lead});
  return replyText;
}
catch(err){
  logger.error({err}, "Error generating response Lead Initialization Chain");
  throw err
}

};

export async function iterationAgent(data: Record<string, string | null>, redisClient: RedisClientType): Promise<{llmOutput: string; redisFlush: boolean}> {

  const systemPrompt = fs.readFileSync('src/static/systemLeadHandlerPrompt.txt', 'utf-8');

   const prompt = ChatPromptTemplate.fromMessages([
  ["system", systemPrompt], 
  ["user",   "Here is the conversation data, the suggestiosn with the original lead data and the  conversation with the lead handler use it to change your response according the sms response:\n{messageHistory}\n\n"]
]);

const llm = new ChatOpenAI({
  apiKey: process.env.OPENAI_API_KEY,
  model:  "gpt-5-mini",
});
const toText = new StringOutputParser();

const validation = new outputValidation(redisClient);

const chain = RunnableSequence.from([prompt, llm, toText, validation]);

try{
const examples = fs.readFileSync('src/static/examples.txt', 'utf-8');

const replyText: {llmOutput: string; redisFlush: boolean} = await chain.invoke({examples: examples, messageHistory: data});

return replyText;

}catch(err)
{
  logger.error({err}," Error generating response in Agentic Chain");
  throw err
}

}

class outputValidation extends Runnable<string,{llmOutput: string; redisFlush: boolean}> {
  
  lc_namespace = ["langChain", "tools",  "outputValidation"];

  async invoke(input: string): Promise<{llmOutput: string; redisFlush: boolean}> {
    logger.info({input},"Input generated by the llm");
    
    if (input.trim().toLowerCase().startsWith("<<ok>>:")) {
      const customResponse = input.split("<<ok>>:")[1].trim();
      return {
        llmOutput: "Message finalized with custom response: \n\n" + customResponse + "\n\nMessage sent to lead by email.",
        redisFlush: true
      };
    }

    return {
      llmOutput: input,
      redisFlush: false
    };
  }
}


